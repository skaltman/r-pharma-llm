{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from chatlas import ChatAuto\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf4572",
   "metadata": {},
   "source": [
    "List models by calling the `list_models` method on a `Chat` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatAuto(\"openai\").list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c3a4a",
   "metadata": {},
   "source": [
    "You can also load the models into a Polars DataFrame for easier viewing.\n",
    "Try it out with different providers: `\"anthropic\"`, `\"ollama/gemma3\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90062112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "models = ChatAuto(\"openai\").list_models()\n",
    "models = pl.DataFrame(models)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b338d",
   "metadata": {},
   "source": [
    "Now try sending the same prompt to different models to compare the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a  recipe for an easy weeknight dinner my kids would like.\"\n",
    "\n",
    "ChatAuto(\"openai/gpt-5\").chat(prompt)\n",
    "ChatAuto(\"anthropic/claude-3-7-sonnet-20250219\").chat(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have local models installed, try them out with Ollama.\n",
    "ChatAuto(\"ollama/gemma3:4b\").chat(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07714be6",
   "metadata": {},
   "source": [
    "Instead of `ChatAuto()`, you can also use the direct provider functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1723b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatlas import ChatAnthropic, ChatOllama, ChatOpenAI\n",
    "\n",
    "ChatOpenAI(model=\"gpt-5\")\n",
    "ChatAnthropic(model=\"claude-3-7-sonnet-20250219\")\n",
    "ChatOllama(model=\"gemma3:4b\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
