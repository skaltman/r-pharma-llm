---
title: Getting Setup
---

{{< include partials/links.qmd >}}

### What to bring

**Bring your [personal laptop]{.text-danger} and a power cord.** This is going to be a hands-on workshop, meaning that youâ€™ll be writing code and collaborating with new friends youâ€™ll make at the workshop.


::: {.callout-important title="You need an API key!"}
We'll be using APIs from [OpenAI][openai-api] and [Anthropic][anthropic-api], so you will need to create an account with each service and get an API key.

Head to [OpenAI][openai-api] and [Anthropic][anthropic-api] to create your accounts and get your API keys.
Once you have your API keys, store them in [the project folder](#clone-the-repository) in a file named `.env`:

```{.env filename=".env"}
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
```

If you're using the exercises in this workshop, it's unlikely that your API usage will exceed a few dollars.
Alternatively, if you want to use or try local models (which are free) during the workshop, you should [install ollama and download a model](#local-models).
:::

<!--
:::::: {.callout-caution title="Bring your own laptop"}
You'll need a computer with internet access and the ability to install R or Python packages.

::::: {.grid .gap-2}
::: {.g-col-12 .g-col-sm-6 .g-col-lg-8}
We **strongly recommend** using a *personal laptop* rather than a work laptop.
Many work laptops have restrictions that may prevent you from installing necessary software or accessing certain websites.

You'll also want to bring a power cord to keep your laptop charged throughout the workshop.
And a bottle of water to stay hydrated (for you, not your laptop)!
:::

::: {.g-col-12 .g-col-sm-6 .g-col-lg-4 .text-end .align-self-center}
![Frustrated man holding his head in his hands and looking confused at his laptop.](assets/frustrated-man.png)
:::
:::::
::::::

::: {.callout-tip title="We'll give you an API key"}
We'll be using APIs from [OpenAI][openai-api] and [Anthropic][anthropic-api], but don't worry, you don't need to bring an API key.
We'll provide an API key for you to use during the workshop and we'll walk you through set up in-person.

If you'd like to also use or try local models during the workshop, you should [install ollama and download a model](#local-models).
:::
-->

## Create Accounts

In this workshop, we'll be using a few different online services.
If you don't already have accounts with these services, you'll need to create them before we get started.

-   [Discord](https://discord.com/register)
    -   Used to communicate during the workshop and ask questions via text. Also used for general online participation during the conference.
    -   Make sure your [display name](https://support.discord.com/hc/en-us/articles/12620128861463-New-Usernames-Display-Names#h_01GXPQABMYGEHGPRJJXJMPHF5C) is the one you used to register for the conference.
    -   Join the `{{< meta workshop.conference.name >}}` Discord server via [the posit::conf event portal]({{< meta workshop.links.conf.href >}}).
    -   Our workshop channel is [`{{< meta workshop.discord_channel >}}`]({{< meta workshop.links.discord >}}).
-   [GitHub](https://github.com/join)
    -   The workshop materials are hosted on GitHub and we may use GitHub in an activity during the workshop.

```{=html}
<!--
* [Posit Cloud](#using-posit-cloud)
-->
```

```{=html}
<!--
## Using Posit Cloud

::: {.callout-note title="Back-up Plan"}
We're providing a Posit Cloud workspace with all of the packages and files ready-to-go for the workshop.

::: lead
Join the Posit Cloud workspace at [{{< meta workshop.links.cloud_short.text >}}]({{< meta workshop.links.cloud_short.href >}}).
:::
:::


::: {.callout-warning title="Posit Cloud Space Not Yet Available"}
The link to join the space is not yet available.
This page will be updated on the day of the workshop.

You will need a Posit Cloud account and you can create one for free at [posit.cloud/plans](https://posit.cloud/plans).
Click "Learn more" under the Free plan to sign up.
:::


I've prepared a Posit Cloud workspace for workshop participants with a project for you to use during our time together.
The project contains all of the files and packages pre-installed and ready to go.
All you need to do is log in and start coding!

If you already have a Posit Cloud account, first [join the {{< meta workshop.links.cloud_short.text >}} space on Posit Cloud]({{< meta workshop.links.cloud_short.href >}}).
Otherwise [sign up for a free account](https://posit.cloud/plans) and then join the space.
You can create an account with your email or login with Google, GitHub or Clever.

Once you're part of the [{{< meta workshop.links.cloud_short.text >}} space]({{< meta workshop.links.cloud_short.href >}}), select the `{{< meta workshop.links.cloud_short.text >}}` assignment to create and launch a new project just for you.

::: {.shadow-lg .p-2 .border .border-1 .rounded-2 .text-center}
TODO: Update posit cloud workspace screenshot
![](assets/posit-cloud-project.png){.img-fluid alt=""}
:::
-->
```

## Choose your IDE

You can use any IDE you like to follow along with the workshop.
We'll be using [Positron](https://positron.posit.co), the free, next-generation data science IDE from Posit.

::: text-center
[![](assets/positron-astropy.png){style="max-width:600px" alt="Positron IDE screenshot"}](https://positron.posit.co)

[<i class="bi bi-display-fill"></i> Download Positron](https://positron.posit.co/download.html){.btn .btn-primary}
:::

Of course, you're welcome to use [RStudio](https://posit.co/download/rstudio-desktop/), [VS Code](https://code.visualstudio.com/) or any other IDE you prefer.

::: {.callout-tip title="Why use Positron?"}
Using Positron offers you the opportunity to try out two new AI-powered features in Positron:

* [Positron Assistant](https://positron.posit.co/assistant), a general purpose AI assistant integrated directly into your development environment.

* [Databot](https://positron.posit.co/databot.html), an AI assistant specifically designed to accelerate exploratory data analysis.
:::

## Prepping for the workshop

To prepare for the workshop, you need to [clone the repository]({{< meta workshop.links.repo >}}) and install the necessary packages.

### Clone the repository

::::::: panel-tabset
#### ![](assets/icons/positron.svg){width="32px"} Positron

In Positron, use **File** \> **New Folder from Git...**.
Enter the repository link---`{{< meta workshop.links.repo >}}.git`---and choose a location on your computer to save the project.

::: text-center
![](assets/positron-new-folder-from-git.png)
:::

#### ![](assets/icons/rstudio.svg){width="32px"} RStudio

In RStudio, use the project dropdown menu (top right) or **File** \> **New Project...**.
Choose **Version Control** and then pick **Git**.

::: text-center
![Step 1: Choose "Version Control" and "Git".](assets/rstudio-new-project-wizard.png){style="max-width:400px"}
:::

Enter the repository link---`{{< meta workshop.links.repo >}}.git`---and choose a location on your computer to save the project.

::: text-center
![Step 2: Enter the repo URL and choose a location for the project.](assets/rstudio-new-project-clone.png){style="max-width:400px"}
:::

#### ![](assets/icons/vscode.png){width="32px"} VS Code

In VS Code,

1.  open the command palette with `Ctrl+Shift+P` (Windows/Linux) or `Cmd+Shift+P` (Mac).
2.  Type `Git: Clone` and select it.
3.  Enter the repository link: `{{< meta workshop.links.repo >}}.git`
4.  Choose a location on your computer to save the project.

::: text-center
![](assets/vscode-git-clone.png){style="max-width:400px"}
:::

#### ![](assets/icons/r-color.svg){width="36px"} usethis

You can use the [usethis](https://usethis.r-lib.org) package to quickly clone the repository:

``` r
usethis::create_from_github(
  "posit-conf-2025/llm",
  # Decide where to put the project here:
  destdir = "~/Desktop/llm"
)
```

This will download the repository and open the project in RStudio.

#### ![](assets/icons/github.svg){width="32px"} GitHub

``` bash
cd ~/Desktop # or somewhere you can find easily

gh repo clone posit-conf-2025/llm
cd llm
```

#### ![](assets/icons/git-icon.svg){width="32px"} git

``` bash
cd ~/Desktop # or somewhere you can find easily

git clone https://github.com/posit-conf-2025/llm.git
cd llm
```
:::::::

### Set up your environment

This workshop is designed so that you can use either R or Python.
You can choose to use only one language throughout, or you can even switch between R and Python during the workshop!

Even if you're only planning to use R, consider installing [`uv`, a package and environment manager for Python](https://docs.astral.sh/uv/) (see the Python instructions below).
That way, could easily try out the Python examples during the workshop if you want to.

:::: {.panel-tabset group="lang"}
#### ![](assets/icons/r-color.svg){height="36px" alt="R"}

First, make sure you're using [a recent version of R](https://r-project.org).
I used [R 4.5](https://r-project.org) but any recent version of R (\>= 4.1) should work.
I also use [rig](https://github.com/r-lib/rig) to manage my R installations, since it makes it easy to install new versions of R and switch between them.

Then, open the project in your IDE and run the following commands in the R console:

```r
# Install pak if you don't have it already
# install.packages("pak")

# Add the Posit and RStudio R-Universe repos for easy dev package installation
pak::repo_add("https://posit-dev.r-universe.dev")
pak::repo_add("https://rstudio.r-universe.dev")

pak::local_install_deps()
```

<details><summary>Alternative: Direct package installation</summary>

```r
{{< include ../_setup.R >}}
```

</details>

#### ![](assets/icons/python-icon-color.svg){height="36px" alt="Python"}

We're using [uv](https://docs.astral.sh/uv/) by Astral to manage our Python environment and dependencies.
If you don't have `uv` installed, you can install it by following [uv's installation instructions](https://docs.astral.sh/uv/getting-started/installation/).

Once you're set up with `uv`, open the project in your IDE and run the following command in the terminal to create the Python environment and install the necessary packages:

``` bash
uv sync
```

That command will create a virtual environment in the project directory and install all the required packages listed in the `pyproject.toml` file.
::::

## Local models with ollama {#local-models}

[ollama is an open-source tool](https://ollama.com) for running LLMs locally on your computer.
Local models do not provide the same quality of responses as flagship models from AI providers like OpenAI or Anthropic, but you can run models on your own computer without having to pay for API access or sending your data to a third party.

**You do not need to install ollama or use local models to participate in the workshop.** That said, they're a fun way to experiment with LLMs without incurring API costs.

To use ollama, first [download and install it on your computer](https://ollama.com/download).
Then, you can browse [the list of models on the ollama website](https://ollama.com/search) to find a model that you want to use.
Here are a few that we like for local testing.
For the workshop, I'd recommend installing `gemma3:4b`[^1].

[^1]: The best open-weights model you can run on a typical laptop (with 16GB of RAM and a decent processor, like an M1 Mac) is \[gpt-oss:20b\]\[ollama-gpt-oss\].
    For something a little bit smaller, try \[qwen3:8b\]\[ollama-qwen3-8b\].

| ID | Size | Capabilities |
|-----------------------:|-----------------------:|:-----------------------|
| \[gemma3:4b\]\[ollama-gemma3-4b\] | 3.3GB | [vision]{.badge .text-bg-danger} |
| \[qwen3:8b\]\[ollama-qwen3-8b\] | 5.2GB | [tools]{.badge .text-bg-primary} [thinking]{.badge .text-bg-info} |
| \[gpt-oss:20b\]\[ollama-gpt-oss\] | 14GB | [tools]{.badge .text-bg-primary} [thinking]{.badge .text-bg-info} |

To use a model, you first need to download it.
For example, to download \[gemma3:4b\]\[ollama-gemma3-4b\], run the following command in your terminal:

``` bash
ollama pull gemma3:4b
```

That command downloads the model to your computer.
Once the model is downloaded, you can use it on your own computer without needing to be connected to the internet.

You can use `ollama` to chat with the model directly in your terminal

``` bash
ollama run gemma3:4b
```

or you can use ellmer or chatlas to interact with the model from R or Python.

::: {.panel-tabset group="lang"}
#### ![](assets/icons/r-color.svg){height="36px" alt="R"}

``` r
library(ellmer)

chat <- chat_ollama(model = "gemma3:4b")
chat$chat("What is the capital of France?")
```

> The capital of France is **Paris**.
>
> It's a global center for art, fashion, gastronomy, and culture.
> ðŸŒðŸ‡«ðŸ‡·

#### ![](assets/icons/python-icon-color.svg){height="36px" alt="Python"}

``` python
from chatlas import ChatOllama

chat = ChatOllama(model="gemma3:4b")
chat.chat("What is the capital of France?")
```

> The capital of France is Paris.
>
> Itâ€™s a global center for art, fashion, gastronomy, and culture.
> ðŸ˜Š
>
> Do you want to know anything more about Paris?
:::

::: {.callout-note title="Why use a local model?" collapse="true"}
Local models are just like the famous ChatGPT or Claude models, but they run entirely on your own computer.
You can use them for all of the same tasks: conversational AI, code generation, text processing, vision and more.
There are a wide range of models available at many different sizes and capabilities, so you can choose one that fits your hardware and performance needs.

**Local models are no where near as capable as the flagship models from OpenAI or Anthropic.** Still, there are a few reasons you might want to use a local model:

1.  Your conversation never leaves your computer, ensuring complete data privacy.
2.  No API costs, making local models very low-cost (other than the cost of your laptop or the hardware to run the model).
3.  They work offline, so you can use them without an internet connection.

Local models are a great option for prototyping, experimentation, and personal use.
They're also useful to companies that want to use LLMs but have strict data privacy requirements and need to ensure that their data never leaves their network.

On the other hand, because you're running the model on your own hardware, local models are slower and less powerful than cloud-based models, especially if you're using a laptop or desktop computer.
Personally, I use local models for experiments and testing to avoid API charges and switch to cloud-based models when I'm putting an LLM-powered application into production.
:::

## The night before the workshop

If you've followed the instructions above, you should be all set for the workshop!
But we'll likely be making some last-minute changes to the workshop materials as we get closer to the event.

To be completely ready-to-go on the day of the workshop, make sure that you get the latest version of the materials the night before the workshop.

-   **Update your local copy of the repository:**

    Use `git pull` in the terminal, or the **Git: Pull** command in your IDE.

-   **Update your R packages:**

    If you're using R, run `renv::restore()` again to make sure you have the latest package versions.

-   **Update your Python packages:**

    If you're using Python, run `uv sync` again to make sure you have the latest package versions.
