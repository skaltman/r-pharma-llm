---
title: Prompt Engineering
subtitle: Getting Started with LLM APIs in R
author: R/Pharma 2025
date: 2025-11-03

editor:
  render-on-save: true
---

# [Prompt engineering]{.white .dib style="text-align: center; width: 100%;"} {.no-invert-dark-mode background-image="assets/kelly-sikkema-tk9RQCq5eQo-unsplash.jpg" background-size="cover" background-position="bottom left"}

::: notes
claude: Prompt engineering is the art of getting LLMs to do what you want. Let's start by exploring what happens when LLMs see plots.
:::

# Your Turn `07_plot-image-1` {.slide-your-turn}

::: notes
claude: First, let's see how well LLMs can interpret real data visualizations. Create a scatter plot and see what the model says.
:::

1. ellmer lets you show the model your plots!

2. Create a basic `mtcars` scatter plot and ask Claude Sonnet 4.5 to interpret it.

3. How does it do? Experiment with other plots!

{{< countdown 3:00 left=0 bottom="-2em" >}}

::: notes
it does pretty well
:::

# Your Turn `08_plot-image-2` {.slide-your-turn}

::: notes
claude: Now let's explore hallucination. Show the model random noise and see if you can get it to admit there's no pattern - this demonstrates the importance of good prompting.
:::

1. Replace the scatter plot with random noise.

2. Show this new plot to Claude Sonnet 4.5 and ask it to interpret it. How does it do this time?

3. Experiment to see if you can improve the prompt to get a better answer.

{{< countdown 7:00 left=0 bottom="-2em" >}}

# [Prompt engineering]{.white .dib style="text-align: center; width: 100%;"} {.no-invert-dark-mode background-image="assets/kelly-sikkema-tk9RQCq5eQo-unsplash.jpg" background-size="cover" background-position="bottom left"}

::: notes
claude: After those exercises, let's talk about prompt engineering best practices. When things don't work, ask yourself these three questions.
:::

## Three questions to ask yourself

::: incremental
1. Did you use the best models?

1. Did you clearly explain what you want the model to do in the system prompt?

1. Did you provide examples of what you want?
:::

::: notes
> There's going to be 3 questions we're usually going to ask you in this order? Number one, did you try the best models? Did you try this with at least like Claude Sonnet 4.5, or GPT 4.1?
>
> so number one like, did you use the best models number 2 is, did you ask for what you wanted in the system prompt like. Did you describe the behavior that you were looking for from the model in the system prompt?
>
>  And then number 3, if you did do that. Did you provide an example of what you wanted? And usually that, that, you know, takes care of a lot of the the kind of misbehaving that people see.
:::

## System prompt vs. user prompt

::: notes
claude: A common question - what goes in the system prompt versus the user prompt? Here's the simple rule.
:::

Short answer: put instructions and background knowledge in the system prompt.

## More tips {.scrollable}

::: notes
claude: Here are more practical tips. First - use LLMs to write your prompts! Claude has a prompt generator that's excellent for this.
:::

**Use LLMs to help draft or improve your prompts.**

E.g., this input to Claude's prompt generator:

> Make a data science agent that can run Python data analysis code via a tool. Make the agent maniacally focused on data quality issues, such as missing data, misspelled categorical values, inconsistent data types, outlier values, impossible values (like negative physical dimensions), etc.

Generates this prompt:

```{.markdown .code-overflow-wrap code-line-numbers="false"}
You are a data science agent with an obsessive focus on data quality. You have been given access to a dataset and a Python code execution tool. Your primary mission is to identify and analyze data quality issues with extreme thoroughness and attention to detail.

Here is the dataset description:
<dataset_description>
{{DATASET_DESCRIPTION}}
</dataset_description>

You have access to the following Python code execution tool:
<python_tool>
{{PYTHON_CODE_TOOL}}
</python_tool>

Your role is to act as a maniacally thorough data quality inspector. You should be suspicious of every aspect of the data and leave no stone unturned in your quest to find data quality issues.

Focus intensively on these categories of data quality problems:
- Missing data (nulls, empty strings, placeholder values like "N/A", "Unknown", etc.)
- Inconsistent data types (mixed types in columns, incorrect data types)
- Misspelled or inconsistent categorical values (typos, case inconsistencies, extra spaces)
- Outlier values (statistical outliers, values that seem unreasonable)
- Impossible or illogical values (negative ages, future birth dates, negative physical dimensions)
- Duplicate records or near-duplicates
- Inconsistent formatting (date formats, phone numbers, addresses)
- Data entry errors (obvious typos, transposed digits)
- Referential integrity issues (if applicable)
- Range violations (values outside expected bounds)

Your analysis process should be systematic and comprehensive:

1. Start by loading and examining the basic structure of the dataset
2. Check data types and identify any type inconsistencies
3. Examine missing data patterns thoroughly
4. Analyze each column individually for quality issues specific to its data type
5. Look for statistical outliers and impossible values
6. Check for duplicates and near-duplicates
7. Examine categorical variables for inconsistencies
8. Validate logical relationships between columns
9. Look for formatting inconsistencies

Use the Python tool to write and execute code that will help you uncover these issues. Be creative in your analysis - write code to check for subtle problems that others might miss.

Before providing your final analysis, use the scratchpad to plan your investigation strategy:

<scratchpad>
[Plan your systematic approach to analyzing the data quality, thinking through what specific checks you want to perform and in what order]
</scratchpad>

Then execute your analysis using the Python tool. After completing your investigation, provide your findings in this format:

<data_quality_report>
**CRITICAL ISSUES FOUND:**
[List the most severe data quality problems]

**MODERATE ISSUES FOUND:**
[List issues that should be addressed but aren't critical]

**MINOR ISSUES FOUND:**
[List smaller issues that could be improved]

**DETAILED ANALYSIS:**
[Provide detailed explanations of each issue found, including specific examples and the potential impact]

**RECOMMENDATIONS:**
[Provide specific, actionable recommendations for fixing each category of issues]

**DATA QUALITY SCORE:**
[Provide an overall data quality score from 1-10, where 10 is perfect quality]
</data_quality_report>

Remember: Be obsessively thorough. Assume there are data quality issues hiding in the dataset and don't stop until you've found them. Question everything and trust nothing until you've verified it through code analysis. Your reputation depends on catching every possible data quality issue.
```

## More tips {.scrollable}

::: notes
claude: Structure matters. Use Markdown headings and XML tags to organize your prompts. And you can use variables for dynamic content.
:::

- **Use Markdown headings and XML tags to give structure to your prompts.**
- **Use variables to insert dynamic content into your prompts--BUT be aware of prompt injection!**

```markdown
Your task is to provide feedback on a research paper summary.
Here is a summary of a medical research paper:
<summary>
{{SUMMARY}}
</summary>

Here is the research paper:
<paper>
{{RESEARCH_PAPER}}
</paper>

Review this summary for accuracy, clarity, and completeness on
a graded A-F scale.
```

## More tips

::: notes
claude: When prompts get long, move them to separate files. This makes your code cleaner and version control easier to read.
:::

**Get large prompts out of the code and into separate files.**

- Easier to read (both locally and on GitHub)

- Easier to read diffs in version control

- We will do this in one of our exercises later

## More tips

::: notes
claude: Advanced technique - make the model think out loud. This helps it follow complex rules or constraints.
:::

**Force the model to say things out loud.**

E.g., "Use no more than three rounds of tool calls" => "Before answering, note how many tool calls you have made inside <memo></memo> tags. If you have made three, stop and answer."

## More tips

::: notes
claude: Finally, all the major providers have excellent documentation on prompt engineering. These are great resources to dive deeper.
:::

See Anthropic's [_Prompt Engineering Overview_](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview) and OpenAI's [_OpenAI Cookbook_](https://cookbook.openai.com/) are excellent, and contain lots of tips and examples.

Google's [_Prompt Design Strategies_](https://ai.google.dev/gemini-api/docs/prompting-strategies) may also be useful.

# Your Turn `09_quiz-game-1` {.slide-your-turn}

::: notes
claude: Now put it all together - write a system prompt for a quiz game. This is a complex task that requires clear instructions and structure.
:::

::: {style="width: 90%;"}
1. Your job: teach the model to play a quiz game with you:

1. The user picks a theme from a short list provided by the model.

1. They then answer multiple choice questions on that theme.

1. After each question, tell the user if they were right or wrong and why.
   Then go to the next question.

1. After 5 questions, end the round and tell the user they won, regardless of their score.
   Then, start a new round.
:::

{{< countdown 8:00 left=0 bottom="-1em" >}}
