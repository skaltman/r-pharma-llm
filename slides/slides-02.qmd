---
title: Anatomy of a conversation
subtitle: Getting Started with LLM APIs in R
author: R/Pharma 2025
date: 2025-09-16

editor:
  render-on-save: true
---

```{r}
# REMOVE
source("_incremental_slides.R")
```

# [Anatomy of a Conversation]{.white} {.title-push-down .no-invert-dark-mode background-image="assets/jaroslaw-glogowski-oAYKzeP0qF4-unsplash.jpg" background-size="cover" background-position="left 30%"}

## {.center}

![](assets/cat-fact-1.png)
![](assets/cat-fact-2.png){.fragment}

::: notes
Those conversations often look something like this:

You ask a question and ChatGPT responds.
You can keep talking to ChatGPT and it keeps responding,
almost like you're having a conversation with a person.

The entire conversation happens via HTTP requests and responses
:::

## What's an HTTP request?

::: notes
HTTP requests power the internet.
When you visit a website, your browser sends an HTTP request to the server hosting that website.
The server responds to the request by sending back the HTML of the webpage.
And then your browser makes a bunch more requests to get the images, stylesheets, and scripts that make up the page, etc.
:::

```{r}
#| output: asis

incremental_slides(
  pattern = "intro-conversation-google-.+[.]svg$",
  template = r"(::: {{.mt6}}
{{{{< include {path} >}}}}
:::)",
  collapse = "\n\n## What's an HTTP request?\n\n"
)
```

## Talking with ChatGPT happens via HTTP

::: notes
Talking to an LLM like ChatGPT also happens via HTTP requests.
When you send a message to ChatGPT, you're sending an HTTP request to OpenAI's servers, this time a `POST` request.

OpenAI processes the request, runs the model, gets the answer and sends it back to you as the response to that POST request.
:::

```{r}
#| output: asis

incremental_slides(
  pattern = "intro-conversation-chat-.+[.]svg$",
  template = r"(::: {{.mt6}}
{{{{< include {path} >}}}}
:::
  )",
  collapse = "\n\n## Talking with ChatGPT happens via HTTP\n\n"
)
```

## Messages have roles

```{r}
#| output: asis

incremental_slides(
  pattern = "intro-conversation-roles-.+[.]svg$",
  template = r"(::: {{.tc}}
{{{{< include {path} >}}}}
:::)",
  collapse = "\n\n## Messages have roles\n\n"
)
```

## Message roles

| Role        | Description                                                       |
|:-----------|:---------------------------------------------------------------|
| `system_prompt` | Instructions from the developer (that's you!)<br>to set the behavior of the assistant |
| `user`       | Messages from the person interacting<br>with the assistant        |
| `assistant`  | The AI model's responses to the user                           |

## {.center}

![](../assets/logos/ellmer.png){style="max-width: 100%; max-height: 500px; display: block; margin-left: auto; margin-right: auto;"}

## {transition="fade"}

::: {style="--code-font-size: 0.9em"}
```r
library(ellmer)
```
:::

## {transition="fade"}

::: {style="--code-font-size: 0.9em"}
```{.r code-line-numbers="3"}
library(ellmer)

chat <- chat_openai()
```
:::

## {transition="fade"}

::: {style="--code-font-size: 0.9em"}
```{.r code-line-numbers="5"}
library(ellmer)

chat <- chat_openai()

chat$chat("Tell me a quick fact about sheep.")
```
:::

## {transition="fade"}

::: {style="--code-font-size: 0.9em"}
```{.r code-line-numbers="5-8"}
library(ellmer)

chat <- chat_openai()

chat$chat("Tell me a quick fact about sheep.")
#> Sheep have excellent memoriesâ€”they can remember up
#> to 50 individual sheep faces and even recognize 
#> human faces for years!
```
:::

::: {.fragment .absolute top=30 right=30 class="w-50 ba b--dark-red bg-washed-red dark-red pa3 br2 flex items-start tr"}
â“ What are the **user** and **assistant** roles in this example?
:::

## {transition="fade"}

::: {style="--code-font-size: 0.9em"}
```r
chat
```
```{.markdown code-line-numbers=false}
<Chat OpenAI/gpt-4.1 turns=2 tokens=15/23 $0.00>
â”€â”€ user [15] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tell me a quick fact about sheep
â”€â”€ assistant [23] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sheep have excellent memoriesâ€”they can remember up to 50 individual sheep faces and even recognize human faces for years!
```
:::

::: {.fragment .absolute top=100 right=50 class="w-50 ba b--dark-red bg-washed-red dark-red pa3 br2 flex items-start tr"}
â“ What about the **system prompt?**
:::

## {transition="fade"}

::: {.smaller style="--code-font-size: 0.9em"}
```{.r code-line-numbers="3-5"}
library(ellmer)

chat <- chat_openai(
  system_prompt = "Always answer in haikus."
)

chat$chat("What is chirality?")
```
:::

## {transition="fade"}

::: {.smaller style="--code-font-size: 0.9em"}

```{.r code-line-numbers="7"}
library(ellmer)

chat <- chat_openai(
  system_prompt = "Always answer in haikus."
)

chat$chat("What is chirality?")
```

::: fragment
```{.markdown code-line-numbers=false}
Left and right differâ€”  
mirrored, yet not the same thing.  
Chirality's twist.
```
:::
:::

## {transition="fade"}

::: {.smaller style="--code-font-size: 0.9em"}
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```{.r code-line-numbers="7"}
chat
```

::: fragment
```{.markdown code-line-numbers=false}
<Chat OpenAI/gpt-4.1 turns=3 tokens=22/22 $0.00>
â”€â”€ system [0] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Always answer in haikus.
â”€â”€ user [22] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
What is chirality?
â”€â”€ assistant [22] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Left and right differâ€”  
mirrored, yet not the same thing.  
Chirality's twist.
```
:::
:::


# Your Turn `02_word-game` {.slide-your-turn}

1. Set up a `chat` with a system prompt:

   > You are playing a word guessing game. At each turn, guess the word and tell us what it is.

2. **Ask:** _In British English, guess the word for the person who lives next door._

3. **Ask:** _What helps a car move smoothly down the road?_

4. Create a new, empty `chat` and ask the second question again.

5. How do the answers to 3 and 4 differ? Why?

{{< countdown 5:00 left=0 bottom="-2em" >}}

# Demo: `clearbot` {.slide-demo style="--code-font-size: 0.66em"}

ğŸ‘©â€ğŸ’» [_demos/03_clearbot/app.py]{.code .b .purple}

**System prompt:**

```{.markdown code-line-numbers=false}
You are playing a word guessing game. At each turn, guess the word and tell us what it is.
```

**First question:**

```{.markdown code-line-numbers=false}
In British English, guess the word for the person who lives next door.
```

**Second question:**

```{.markdown code-line-numbers=false}
What helps a car move smoothly down the road?
```

# [How LLMs work (briefly)]{.dib .bg-black .ph4 .white style="position: relative; top: -3em;"} {.no-invert-dark-mode background-image="assets/andy-kelly-0E_vhMVqL9g-unsplash.jpg" background-size="cover" background-position="center"}

## {.center}

![](assets/intro-conversation-01.png)
![](assets/intro-conversation-02.png){.fragment}
![](assets/intro-conversation-03.png){.fragment}
![](assets/intro-conversation-04.png){.fragment}

::: notes
Those conversations often look something like this:

You ask a question and ChatGPT responds.
You can keep talking to ChatGPT and it keeps responding,
almost like you're having a conversation with a person.
:::

## [but is it really a conversation?...]{.dib .bg-black .ph4 .white} {.no-invert-dark-mode background-image="assets/marija-zaric-wFWXRDilwKo-unsplash.jpg" background-size="cover" background-position="center"}

::: notes
But this is not a conversation.

It certainly _feels_ like a conversation,
but something else is happening behind the scenes.
:::


## When you talk to ChatGPT {.center}

::: incremental
1. You send words to ChatGPT

2. ChatGPT sends words back to you

3. It feels like you're having a conversation
:::


## ChatGPT {.center}

::: r-fit-text
**Chat**ting with a **G**enerative **P**re-trained **T**ransformer
:::

::: {.fragment .r-fit-text}
**LLM** &rarr; **L**arge **L**anguage **M**odel
:::

::: notes
GPT is a type of LLM, but not all LLMs are GPTs.

LLM: Deep learning on huge amounts of text data to understand and generate human language.
GPT: Specific architectural design of LLMs.

From now on, we'll use **LLM** to refer to any large language model.
:::

## How to make an LLM {.center}

::: {.columns}
::: {.column}
**If you read everything<br>ever written...**

* Books and stories

* Websites and articles

* Poems and jokes

* Questions and answers
:::
::: {.column .fragment}
<br>**...then you could...**

- Answer questions
- Write stories
- Tell jokes
- Explain things
- Translate into any language
:::
:::


## The cat sat in the ____ {.center}

::: incremental
::: {.columns style="font-size: 6rem;"}
::: {.column}
* ğŸ©
* ğŸ›Œ
* ğŸ“¦
:::
::: {.column}
* ğŸªŸ
* ğŸ›’
* ğŸ‘ 
:::
:::
:::

::: notes
The breakthrough insight is the attention mechanism in the Transformer architecture.
Instead of processing text sequentially (word by word),
GPT models can look at all words in a sequence simultaneously
and understand how they relate to each other.

The attention mechanism allows the model to:

* Weigh the importance of every word in relation to every other word
* Capture long-range dependencies that traditional models missed
* Process sequences in parallel rather than sequentially
:::

## Actually: tokens, not words {.center}

::: {.incremental}
- Fundamental units of information for LLMs
- Words, parts of words, or individual characters
  - "hello" â†’ 1 token
  - "unconventional" â†’ 3 tokens: `un|con|ventional`
- Important for:
  - Model input/output limits
  - [API pricing](https://llmpricecheck.com/calculator/) is usually by token
- Not just words, but images can be tokenized too
:::

## {background-image="assets/token-approximations.png" background-size="contain" visibility="hidden"}

::: footer
<https://llm-stats.com/>
:::

::: notes
The newest and biggest models from OpenAI and Google
can handle 1 million input tokens at once.

Which is kind of like saying that you could paste

* 30 hours of podcasts
* 1,000 pages of a book
* 60,000 lines of code

into the chat window and the model could "pay attention" to it all.

1M is the upper limit currently, most models accept up to around 200k tokens.
:::


# Demo: <br> `token-possibilities` {.slide-demo style="--code-font-size: 0.66em"}

ğŸ‘©â€ğŸ’» [_demos/04_token-possibilities/app.R]{.code .b .purple}

# [What if I want to keep chatting back-and-forth?]{.bg-white .f1 .normal .relative style="top: -14px"} {background-image="assets/shinychat-input-empty.png" background-size="contain" background-position="center"}

# [ellmer can do that, too!]{.bg-white .f1 .normal .relative style="top: -14px"} {background-image="assets/shinychat-input-focused.png" background-size="contain" background-position="center"}

## {.center}

::: {.table-no-border}
| | Console | Browser |
|:---:|:---:|:---:|
| ![](../assets/logos/ellmer.png){height="4em" alt="ellmer"} | `live_console(chat)` | `live_browser(chat)` |
: {tbl-colwidths="[20,40,40]"}
:::

::: notes
As we'll see, everything we're going to do today can be done
in either R or Python and ellmer and chatlas are designed to be similar.

But they're in different languages and those languages have different
conventions and idioms, so they're not completely identical. If they were,
one or the other would start to feel unnatural to use.
:::

# Demo: <br> `token-possibilities` {.slide-demo style="--code-font-size: 0.66em"}

ğŸ‘©â€ğŸ’» [_demos/05_live/05_live.R]{.code .b .purple}


# How to think about LLMs {.dark-blue background-image="assets/retro-mac.jpg" background-size="cover" background-position="bottom left"}

```{r}
source(here::here("website/slides/_incremental_slides.R"))
```

::: notes
Some motivational words by Joe urging everyone to approach LLMs with curiosity and experimentation rather than preconceived notions about limitations, while building understanding from the ground up through practical experience.
:::

## Think Empirically, Not Theoretically {.center .text-center}

## Think Empirically, Not Theoretically {.center .text-center}


::: {.incremental}
- It's okay to (mostly) treat LLMs as **black boxes**.

- **Just try it!** When wondering if an LLM can do something,\
  experiment rather than theorize

- You might think they could not possibly do things\
  _that they clearly can do today_

- And you might think _surely_ they can do something\
  that it turns out they're _terrible_ at
:::

::: notes
- Understanding the technical details can lead to **bad intuition** about capabilities
- You might think "they could not possibly do things that they clearly can do today"
- Empirical testing reveals actual capabilities vs. theoretical limitations
:::

## LLMs are jagged {.center}

![](assets/model-performance-smooth.png){style="max-width: 90%; max-height: 700px; display: block; margin-left: auto; margin-right: auto;"}

## LLMs are jagged {.center}

![](assets/model-performance-jagged.png){style="max-width: 90%; max-height: 700px; display: block; margin-left: auto; margin-right: auto;"}

# [Embrace the Experimental Process]{.dib .bg-black .ph4 .white style="position: relative; top: 5em;"} {.no-invert-dark-mode background-image="assets/national-cancer-institute-Okk-eID2Z9k-unsplash.jpg" background-size="cover" background-position="center"}

## Embrace the Experimental Process {.center .text-center}

- **Explore!** \
  Focus on learning and engaging with the technology, not outcomes

- **Failure is valuable!** \
  _those are some of the most interesting conversations that we have_

- **It doesn't have to be a success.** \
  Attempts that don't work still provide insights

::: notes
- Use the **best models available** for experimentation
- Don't constrain yourself to "practical" applications initially
- Think of it as forming your own independent conclusions about usefulness
:::

## Start Simple, Build Understanding {.center .text-center}

## Start Simple, Build Understanding {.center .text-center}

- We're going to focus on the **core building blocks**.

- All the incredible things you see AI do \
  **decompose to just a few key ingredients**.

- Our goal is to **have fun and build intuition** \
  through hands-on experience.

::: notes
- Think **very generally** about what tools can do - they're as powerful as any software you can write
- Remember: **"Everything decomposes"** to the basic components once you understand the underlying APIs
- Build intuition through hands-on experience rather than theoretical study
- We want to have fun in this course! It's worth saying up front that our examples are not necessarily practical, but they're full of practical lessons.
:::


# [shinychat]{.hidden} {.no-invert-dark-mode background-image="assets/shinychat.png" background-size="contain" background-position="center"}

## {.center}

::: {.easy-columns .tc}
![](../assets/logos/shinychat.png){style="max-width: 100%; max-height: 500px" alt="shinychat"}

![](../assets/logos/ellmer.png){style="max-width: 100%; max-height: 500px" alt="ellmer"}
:::

## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

```{=html}
<style>
:root {
  --shinychat-code-font-size: 0.66em;
}
</style>
```

Start with the `shinyapp` snippet

```{.r}
library(shiny)
library(bslib)

ui <- page_fillable(

)

server <- function(input, output, session) {

}

shinyApp(ui, server)
```

::: footer
<https://www.garrickadenbuie.com/blog/shiny-new-bslib-snippet/>
:::

## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Load `{shinychat}` and `{ellmer}`

```{.r code-line-numbers="3-4"}
library(shiny)
library(bslib)
library(shinychat)
library(ellmer)

ui <- page_fillable(

)

server <- function(input, output, session) {

}

shinyApp(ui, server)
```

## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Use the shinychat chat module

```{.r code-line-numbers="7,11"}
library(shiny)
library(bslib)
library(shinychat)
library(ellmer)

ui <- page_fillable(
  chat_mod_ui("chat")
)

server <- function(input, output, session) {
  chat_mod_server("chat")
}

shinyApp(ui, server)
```

## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Create and hook up a chat client to use in the app

```{.r code-line-numbers="12-13"}
library(shiny)
library(bslib)
library(shinychat)
library(ellmer)


ui <- page_fillable(
  chat_mod_ui("chat")
)

server <- function(input, output, session) {
  client <- chat_openai()
  chat_mod_server("chat", client)
}

shinyApp(ui, server)
```

<!--
## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Why is this not a great idea?

```{.r code-line-numbers="6-14"}
library(shiny)
library(bslib)
library(shinychat)
library(ellmer)

client <- chat_openai()

ui <- page_fillable(
  chat_mod_ui("chat")
)

server <- function(input, output, session) {
  chat_mod_server("chat", client)
}

shinyApp(ui, server)
```

::: notes
Copy and paste the app code into an R session and run it!
:::

-->

# Your Turn `06_word-games` {.slide-your-turn}

1. I've set up the basic Shiny app snippet and a system prompt.

2. Your job: create a chatbot that plays the word guessing game with you.

3. The twist: this time, you're guessing the word.

{{< countdown 7:00 top="-1em" >}}

<!--
## Interpolation

```{r}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "3|5-7"

library(ellmer)

words <- c("elephant", "bicycle", "sandwich")

interpolate(
  "The secret word is {{ sample(words, 1) }}."
)
```

## Interpolation

```{r}
#| echo: true
#| code-line-numbers: "5-7"

library(ellmer)

words <- c("elephant", "bicycle", "sandwich")

interpolate(
  "The secret word is {{ words }}."
)
```

-->


